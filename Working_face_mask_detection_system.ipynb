{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from os import getcwd\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization,Activation, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import imutils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1376\n",
      "Percentage of positive examples: 50.145348837209305%, number of pos examples: 690\n",
      "Percentage of negative examples: 49.854651162790695%, number of neg examples: 686\n"
     ]
    }
   ],
   "source": [
    "def data_summary(main_path):\n",
    "\n",
    "    withmask_path = main_path+'withmaskreal'\n",
    "\n",
    "    withoutmask_path = main_path+'withoutmaskreal'\n",
    "\n",
    "    # number of (images) that are in the the folder named 'with_mask' \n",
    "\n",
    "    m_pos = len(listdir('./observations-master/experiements/data/with_mask'))\n",
    "\n",
    "    # number of (images) that are in the the folder named 'without_mask' \n",
    "\n",
    "    m_neg = len(listdir('./observations-master/experiements/data/without_mask'))\n",
    "\n",
    "    # number of all examples\n",
    "\n",
    "    m = (m_pos+m_neg)\n",
    "\n",
    "    pos_prec = (m_pos* 100.0)/ m\n",
    "\n",
    "    neg_prec = (m_neg* 100.0)/ m\n",
    "\n",
    "    print(f\"Number of examples: {m}\")\n",
    "\n",
    "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "\n",
    "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\")    \n",
    "augmented_data_path = 'facemask-dataset/trial1/augmented data1/'    \n",
    "\n",
    "data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "\n",
    "    dataset = []  \n",
    "\n",
    "    for unitData in os.listdir(SOURCE):\n",
    "\n",
    "        data = SOURCE + unitData\n",
    "\n",
    "        if(os.path.getsize(data) > 0):\n",
    "\n",
    "            dataset.append(unitData)\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Skipped ' + unitData)\n",
    "\n",
    "            print('Invalid file i.e zero size')  \n",
    "\n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "\n",
    "    train_set = dataset[0:train_set_length]\n",
    "\n",
    "    test_set = dataset[-test_set_length:]     \n",
    "\n",
    "    for unitData in train_set:\n",
    "\n",
    "        temp_train_set = SOURCE + unitData\n",
    "\n",
    "        final_train_set = TRAINING + unitData\n",
    "\n",
    "        copyfile(temp_train_set, final_train_set)  \n",
    "\n",
    "    for unitData in test_set:\n",
    "\n",
    "        temp_test_set = SOURCE + unitData\n",
    "\n",
    "        final_test_set = TESTING + unitData\n",
    "\n",
    "        copyfile(temp_test_set, final_test_set)      \n",
    "\n",
    "WITHMASK_SOURCE_DIR = \"./observations-master/experiements/dest_folder/val/with_mask/\"\n",
    "\n",
    "TRAINING_WITHMASK_DIR = \"./observations-master/experiements/dest_folder/train/with_mask/\"\n",
    "\n",
    "TESTING_WITHMASK_DIR = \"./observations-master/experiements/dest_folder/test/without_mask/\"\n",
    "\n",
    "WITHOUTMASK_SOURCE_DIR = \"./observations-master/experiements/dest_folder/val/without_mask/\"\n",
    "\n",
    "TRAINING_WITHOUTMASK_DIR = \"./observations-master/experiements/dest_folder/train/without_mask/\"\n",
    "\n",
    "TESTING_WITHOUTMASK_DIR = \"./observations-master/experiements/dest_folder/test/without_mask/\"\n",
    "\n",
    "split_size = .8\n",
    "\n",
    "split_data(WITHMASK_SOURCE_DIR, TRAINING_WITHMASK_DIR, \n",
    "\n",
    "TESTING_WITHMASK_DIR, split_size)\n",
    "\n",
    "split_data(WITHOUTMASK_SOURCE_DIR, TRAINING_WITHOUTMASK_DIR, \n",
    "\n",
    "TESTING_WITHOUTMASK_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images with facemask in the training set'with_mask': 714\n",
      "number of images with facemask in the test set'with_mask': 97\n",
      "number of images without facemask in the training set'without_mask': 713\n",
      "number of images without facemask in the test set'without_mask': 127\n"
     ]
    }
   ],
   "source": [
    "print(\"number of images with facemask in the training set'with_mask':\", len(os.listdir('./observations-master/experiements//dest_folder//train//with_mask')))\n",
    "\n",
    "print(\"number of images with facemask in the test set'with_mask':\", len(os.listdir('./observations-master/experiements//dest_folder//test//with_mask')))\n",
    "\n",
    "print(\"number of images without facemask in the training set'without_mask':\", len(os.listdir('./observations-master/experiements//dest_folder//train//without_mask')))\n",
    "\n",
    "print(\"number of images without facemask in the test set'without_mask':\", len(os.listdir('./observations-master/experiements//dest_folder//test//without_mask')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', \n",
    "\n",
    "    input_shape=(150, 150, 3)),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1427 images belonging to 2 classes.\n",
      "Found 224 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"./observations-master/experiements/dest_folder/train\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "\n",
    "                                   rotation_range=40,\n",
    "\n",
    "                                   width_shift_range=0.2,\n",
    "\n",
    "                                   height_shift_range=0.2,\n",
    "\n",
    "                                   shear_range=0.2,\n",
    "\n",
    "                                   zoom_range=0.2,\n",
    "\n",
    "                                   horizontal_flip=True,\n",
    "\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, batch_size=10, target_size=(150, 150))\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION_DIR = \"./observations-master/experiements/dest_folder/test\"\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "\n",
    "                                                         batch_size=10, \n",
    "\n",
    "                                                         target_size=(150, 150))\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model'\n",
    "\n",
    ",monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-b52a8546aa48>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "143/143 [==============================] - ETA: 0s - loss: 0.5158 - acc: 0.7631WARNING:tensorflow:From /home/bhavya/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/bhavya/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model/assets\n",
      "143/143 [==============================] - 122s 855ms/step - loss: 0.5158 - acc: 0.7631 - val_loss: 0.4978 - val_acc: 0.8393\n",
      "Epoch 2/30\n",
      "143/143 [==============================] - ETA: 0s - loss: 0.3079 - acc: 0.8858INFO:tensorflow:Assets written to: model-002.model/assets\n",
      "143/143 [==============================] - 145s 1s/step - loss: 0.3079 - acc: 0.8858 - val_loss: 0.3265 - val_acc: 0.9018\n",
      "Epoch 3/30\n",
      "143/143 [==============================] - 129s 903ms/step - loss: 0.2539 - acc: 0.9054 - val_loss: 0.4390 - val_acc: 0.8884\n",
      "Epoch 4/30\n",
      "143/143 [==============================] - 123s 858ms/step - loss: 0.2217 - acc: 0.9075 - val_loss: 0.6298 - val_acc: 0.8795\n",
      "Epoch 5/30\n",
      "143/143 [==============================] - 141s 989ms/step - loss: 0.2761 - acc: 0.9033 - val_loss: 0.4102 - val_acc: 0.9018\n",
      "Epoch 6/30\n",
      "143/143 [==============================] - 142s 990ms/step - loss: 0.2466 - acc: 0.9054 - val_loss: 0.6761 - val_acc: 0.8080\n",
      "Epoch 7/30\n",
      "143/143 [==============================] - 140s 976ms/step - loss: 0.2157 - acc: 0.9173 - val_loss: 0.4377 - val_acc: 0.8973\n",
      "Epoch 8/30\n",
      "143/143 [==============================] - 147s 1s/step - loss: 0.1797 - acc: 0.9243 - val_loss: 0.5686 - val_acc: 0.8973\n",
      "Epoch 9/30\n",
      "143/143 [==============================] - ETA: 0s - loss: 0.1769 - acc: 0.9411INFO:tensorflow:Assets written to: model-009.model/assets\n",
      "143/143 [==============================] - 141s 988ms/step - loss: 0.1769 - acc: 0.9411 - val_loss: 0.3023 - val_acc: 0.9286\n",
      "Epoch 10/30\n",
      "143/143 [==============================] - 139s 971ms/step - loss: 0.2023 - acc: 0.9306 - val_loss: 0.3784 - val_acc: 0.9196\n",
      "Epoch 11/30\n",
      "143/143 [==============================] - 138s 962ms/step - loss: 0.1715 - acc: 0.9348 - val_loss: 0.4058 - val_acc: 0.9152\n",
      "Epoch 12/30\n",
      "143/143 [==============================] - 143s 998ms/step - loss: 0.1773 - acc: 0.9369 - val_loss: 0.3483 - val_acc: 0.9152\n",
      "Epoch 13/30\n",
      "143/143 [==============================] - 141s 983ms/step - loss: 0.1774 - acc: 0.9313 - val_loss: 0.3783 - val_acc: 0.9196\n",
      "Epoch 14/30\n",
      "143/143 [==============================] - 147s 1s/step - loss: 0.1562 - acc: 0.9390 - val_loss: 0.4844 - val_acc: 0.9196\n",
      "Epoch 15/30\n",
      "143/143 [==============================] - 139s 973ms/step - loss: 0.1276 - acc: 0.9530 - val_loss: 0.8601 - val_acc: 0.8661\n",
      "Epoch 16/30\n",
      "143/143 [==============================] - 140s 982ms/step - loss: 0.1214 - acc: 0.9629 - val_loss: 0.7510 - val_acc: 0.8616\n",
      "Epoch 17/30\n",
      "143/143 [==============================] - 141s 988ms/step - loss: 0.1777 - acc: 0.9362 - val_loss: 0.4805 - val_acc: 0.9152\n",
      "Epoch 18/30\n",
      "143/143 [==============================] - 139s 972ms/step - loss: 0.1364 - acc: 0.9502 - val_loss: 0.4631 - val_acc: 0.9241\n",
      "Epoch 19/30\n",
      "143/143 [==============================] - 139s 975ms/step - loss: 0.1220 - acc: 0.9502 - val_loss: 0.7053 - val_acc: 0.9107\n",
      "Epoch 20/30\n",
      "143/143 [==============================] - 141s 984ms/step - loss: 0.1267 - acc: 0.9516 - val_loss: 0.4063 - val_acc: 0.9286\n",
      "Epoch 21/30\n",
      "143/143 [==============================] - 141s 987ms/step - loss: 0.1491 - acc: 0.9418 - val_loss: 0.4115 - val_acc: 0.9286\n",
      "Epoch 22/30\n",
      "143/143 [==============================] - 145s 1s/step - loss: 0.2078 - acc: 0.9292 - val_loss: 0.3063 - val_acc: 0.9062\n",
      "Epoch 23/30\n",
      "143/143 [==============================] - 141s 988ms/step - loss: 0.1267 - acc: 0.9601 - val_loss: 0.3560 - val_acc: 0.9286\n",
      "Epoch 24/30\n",
      "143/143 [==============================] - 174s 1s/step - loss: 0.1222 - acc: 0.9580 - val_loss: 0.7525 - val_acc: 0.9062\n",
      "Epoch 25/30\n",
      "143/143 [==============================] - 172s 1s/step - loss: 0.1447 - acc: 0.9495 - val_loss: 0.6540 - val_acc: 0.9018\n",
      "Epoch 26/30\n",
      "143/143 [==============================] - 143s 1000ms/step - loss: 0.1124 - acc: 0.9580 - val_loss: 0.4881 - val_acc: 0.9286\n",
      "Epoch 27/30\n",
      "143/143 [==============================] - 139s 973ms/step - loss: 0.1196 - acc: 0.9601 - val_loss: 0.4932 - val_acc: 0.9241\n",
      "Epoch 28/30\n",
      "143/143 [==============================] - 139s 973ms/step - loss: 0.1159 - acc: 0.9523 - val_loss: 0.6849 - val_acc: 0.9196\n",
      "Epoch 29/30\n",
      "143/143 [==============================] - 138s 964ms/step - loss: 0.1031 - acc: 0.9566 - val_loss: 0.8290 - val_acc: 0.8973\n",
      "Epoch 30/30\n",
      "143/143 [==============================] - 137s 956ms/step - loss: 0.0976 - acc: 0.9664 - val_loss: 1.0472 - val_acc: 0.8661\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "\n",
    "                              epochs=30,\n",
    "\n",
    "                              validation_data=validation_generator,\n",
    "\n",
    "                              callbacks=[checkpoint])\n",
    "\n",
    "model.save('my_model_facemask.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "labels_dict={0:'without_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
